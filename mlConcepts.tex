
\chapter{Machine Learning: concepts}

When ordinary people hear the words ``Data Science,'' I'll bet the first images
that come to mind are of the closely-related fields of \textbf{data mining} and
\textbf{machine learning (ML)}, even if they don't know those terms. After all,
this is where all the sexy tech is, and the success stories too: Netflix
magically knowing which movies you'll like, grocery chains using data from
loyalty cards to optimally place products; the Oakland A's scouring minor
league stats to build a champion team with chump change
(see:~\textit{Moneyball}). There are also creepier applications of this
technology: Google placing personalized eye-catching ads in front of you using
data they mined from your email text, or Cambridge Analytica projecting from
voter personalities to the best ways to micro-target them.

All these examples have one thing in common: they actually \textit{make} the
discoveries and predictions from the data. They're the coup de gr\^{a}ce. They
take place after we've already acquired our data, imported it to an analysis
environment (like Python), stored it in the appropriate data structures (like
associative arrays or tables), recoded/transformed/pre-processed it as
necessary, and explored it enough to know what we want to ask. All that stuff
was mere prep work. This chapter is where we begin to really rock-and-roll.

\section{Data mining vs.~machine learning}
\index{inference}
\index{prediction}
\index{machine learning (ML)}
\index{data mining}

The terms ``data mining'' and ``ML'' have a lot of sloppy overlap, but one
distinction we can pick out is this. If someone says they're doing data mining,
their goal is normally \textbf{inference}: deriving high-level strategic
insights based on patterns in the data. Discovering that amateur pitching
performances translate more reliably to the major leagues than amateur batting
performances do, generally speaking, is an inference, and a potentially
valuable find.

If someone says they're doing ML, on the other hand, their goal is normally
\textbf{prediction}: making an educated guess about how a specific case will
turn out. When we forecast how many home runs we think a college prospect will
hit in his first two years in the majors, we're making a specific prediction
rather than inferring a general truth -- this, too, is potentially quite
valuable, as it may lead us to decide to sign the player or look at different
options.

\section{Deductive vs.~inductive reasoning}

\index{deductive reasoning}
\index{inductive reasoning}
\index{Holmes, Sherlock}

This chapter contains a lot of vocabulary terms. Before we dive in to the
ML-specific ones, I think it's important to take a step back and make a more
general point about the kind of ``learning'' we'll be doing. There are at least
two different ways that human beings reach conclusions: \textbf{deductively}
and \textbf{inductively}. Deductive reasoning is associated most prominently
with Sherlock Holmes in the public mind. Through sheer application of
irrefutable logic, Holmes and his companion Watson deduced new facts from known
facts in their quest to catch the criminal. Their logic was seemingly
air-tight, since everything they deduced followed directly and irresistibly
from what came before.

There's a subdiscipline of Philosophy called Logic which covers exactly such
matters. Syllogisms, \textit{modus ponens}, first-order predicate calculus:
these are all concepts you'll learn if you take an introductory course in
Logic. And the nice thing about deduction is that as long as you follow the
rules, \textit{your conclusions will always be dependably correct.}

Inductive reasoning, on the other hand, does \textit{not} always lead to 100\%
reliably correct conclusions. This may give you pause, and wonder why anyone
would ever use it. The reason is that in the vast majority of cases, deductive
reasoning simply isn't applicable to your situation, and induction is the only
case.

Induction is about \textit{reasoning from examples}. Lots of examples. Living
in the world as we do, we observe plenty of examples of how people and things
behave, and we start to identify certain general patterns in what we've
observed. One thing I noticed long ago is that when I smile and say hi to a
person, they normally smile and say hi back. But when I smile and say hi to a
dog, or a bush, or a vending machine, I'm normally met with stony silence.

From this, I've \textbf{induced} the general rule that people respond to
greetings but other objects don't. Now this is \textit{not} 100\% reliably
true. Even in my own experience, there have been times when I've greeted
someone walking down the hallway and been outright ignored. And for all I know,
there may be some vending machines out there who might respond if someone talks
to them -- with technological advancements in voice recognition and synthesis,
it's probably just a matter of time before they do. But the point is that
\textit{learning this general principle about greetings has served me very well
in life.} I don't normally talk to inanimate objects, but I do to people, and
this has helped me function in society. Even if a rule \textit{isn't} accurate
in absolutely every situation, it can still be very, very important.
 
If you do a quick scan of your brain, I believe you'll find that the vast
majority of the things that you ``know'' about life were arrived at
inductively, rather than deductively. If you ask a friend for money, he'll
probably say yes; if you ask a stranger, he'll probably say no. If your friend
does say yes, he'll probably expect the favor to be returned at a later point;
if the stranger says yes, he probably won't. If you don't study for a test,
you'll probably do poorly, and likewise if you wait until the last day to start
your 5-page paper. None of these conclusions can be proven deductively, and in
fact all of them have exceptions; but not to know these things is to be at a
serious disadvantage in trying to make decisions.

I say all this because \textit{everything in ML is about induction, not
deduction.} As we'll see, the name of the game in ML is looking at lots and
lots of past examples, and making future predictions based on them. It's true
that ``past performance is no guarantee of future success,'' but past
performance \textit{does} tell you \textit{something} valuable about future
possibilities, else there'd be no point in trying to learn from it. And the
fact that we apply our past lessons in altering our future behavior is
undeniable.

\section{Supervised vs.~unsupervised learning}

\index{supervised ML@``supervised'' ML}
\index{unsupervised ML@``unsupervised'' ML}

Now let's dive further down to some more technical and fine-grained
distinctions. There are two main categories of machine ``learning'':
\textbf{supervised} and \textbf{unsupervised}. I think these terms are
ridiculous and misleading, by the way, but they're what we're stuck with so
let's learn what they mean.

\index{target attribute}
\index{objects (of a study)}
\index{mood}
\index{happy@\textsf{happy}}
\index{angry@\textsf{angry}}
\index{frightened@\textsf{frightened}}
\index{defensive@\textsf{defensive}}
\index{embarrassed@\textsf{embarrassed}}

In a supervised learning setting, our goal is to predict the value of some
\textbf{target attribute} of some object of study. As an example, let's say we
want to predict someone's \textit{mood} based only on their facial expression
and body language. ``Mood'' might be a categorical variable with values
``\textsf{happy},'' ``\textsf{angry},'' ``\textsf{bored},'' \textit{etc.}

\index{training data}
\index{squinty eyes}

To do this prediction, we'll use a bunch of previously observed examples. These
past examples, \textit{for which the person's true mood is known}, are
collectively called our \textbf{training data}. We remember seeing one person
with a smile on their face and their eyes slightly squinted, and later
discovered that they were \textsf{happy}. We remember another person with a
smile but with wide open eyes, and learned that they, too, were \textsf{happy}.
We also remember someone with clenched fists and raised eyebrows, and they
turned out to be \textsf{frightened}. A different person with clenched fists
but squinted eyes was later revealed to be \textsf{angry}. And so on.

Supervised machine learning is about how to extrapolate from past examples in a
principled way, in order to make predictions about future examples whose true
value (mood, say) is \textit{not} known. The task is to say, ``okay, there's a
person down the hallway whose face is slightly flushed and whose arms are
tightly crossed. Are they likely to be \textsf{happy}, \textsf{defensive},
\textsf{angry}, \textsf{embarrassed}, or something else? Let's apply what we've
learned from past examples to guess at the answer.''

It's called ``supervised'' precisely because the ``true answer'' for the target
attribute is known for the training data.

Now suppose we \textit{didn't} know the true answer for our training examples.
Say we've observed and recorded the eyebrow position, the mouth configuration,
whether the face was flushed or pale or in between, \textit{etc.}, for a bunch
of people we've encountered in the past, but we actually never learned what
their mood was. What then?

This is an unsupervised learning setting. Predicting a person's mood based on
this kind of information turns out to be nearly hopeless. If we don't know what
anyone else's mood was, how can we predict what this new person's mood is? But
all is not lost -- we may still be able to form some conclusions about what
\textit{types} of moods there are. For example, we might notice that generally
speaking, raised eyebrows tend to be accompanied by certain other indicators.
In many past examples, they've appeared together with an open mouth and a rigid
posture. In other examples, raised eyebrows instead appeared with lips
tightly-pressed together and the forehead slightly tilted forward. We don't
know which moods these collections of features might correspond to, since our
training data didn't have any information about moods. But we might still infer
the presence of a couple of distinct raised-eyebrow moods, since they are so
commonly accompanied by either one of two groups of other features.

\subsection{Classification, regression, and clustering}

\index{prediction}
\index{classification}
\index{classifier}
\index{regression}
\index{scales of measure}
\index{categorical variable}
\index{function}

In the supervised setting, our most common machine learning activities will be
\textbf{classification} and \textbf{regression}. In each one, our job is to
predict the value of the target attribute for a new object, based on the
previous example objects we've seen. The only difference is the scale of
measure of the target variable: if it's categorical, we're performing
classification, and our goal is to build a \textbf{classifier}: an algorithm
(basically, a Python function) that can \textbf{classify} future examples by
guessing their target value. If the target variable is numeric, then we have
regression, and our goal is to make the closest guess we can to the true target
value.

For example, if we have some census and earnings data for a region, and our
goal is to predict whether or not someone in that region will be a homeowner or
a renter, we're performing classification. If our goal instead is to predict
their annual salary, we're doing regression.

\index{clustering}
\index{IMDB}

In the unsupervised setting, the most common task is \textbf{clustering}:
finding groups of related objects, with similar attribute values, in order to
discern how many basic types of objects there are, and what their typical value
ranges are. That's exactly what we did with the mood data, above, in the
absence of information about past moods. Another example would be to look at
the attributes of various movies on IMDB and discern ``what basic types of
films there are.'' We may discover that movies naturally break down into
blockbuster action films, period dramas, romantic comedies, and a few other
common genres. There will always be objects that defy categorization, and exist
on the boundaries of defined clusters, but it's still profoundly insightful to
discover the presence of common patterns that bring structure to the data.
