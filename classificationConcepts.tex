
\chapter{Classification: concepts}

\section{Labeled and unlabeled examples}

\index{classification}
\index{classifier}
\index{label}
\index{unlabeled examples}
\index{labeled examples}
\index{examples, labeled and unlabeled}
\index{input!to a classifier}
\index{output!of a classifier}

In the activity of classification, the \textbf{target} variable we aim to
predict is categorical. We sometimes also call this variable the
\textbf{label}. Since this is a \textit{supervised} process, we are provided
with example objects of study that have known ``true answers.'' These are
called \textbf{labeled examples}. The goal of the activity is to produce good
predictions of the labels for other, \textbf{unlabeled examples}. A program
that can make such predictions, after having studied the labeled examples, is
called a \textbf{classifier}.

\index{feature}
\index{attribute}

The predicted label can be seen as the ``output'' from our classifier. All of
the other variables are essentially the inputs to our process, which we use to
make our predictions. These variables are called \textbf{features} (or
sometimes, \textbf{attributes}).

In terms of Pandas data structures, all these labeled examples will normally
come packaged in a \texttt{DataFrame}. Each row of the \texttt{DataFrame} will
be one labeled example, with its features as columns and its target/label as a
column (traditionally, the rightmost one).

This is illustrated in Figure~\ref{fig:labeledTrainTest}. Here we have some
labeled examples for a data set on NFL fans. Each row represents one fan, and
shows various features of their existence -- how old they are, where they were
born, where they live now, and how many years they've lived in their current
residence. The rightmost column gives the target: the team to whom this fan has
sworn their allegiance. Our aim would be to predict which team a fan might root
for, based on what we know about them. Lest you think this example is
frivolous, consider that a sporting goods company might want to send catalogs
(paper or electronic) to potential customers, and it would probably boost sales
if the cover image of the catalog featured a model wearing apparel from
the customer's favorite team, rather than their rival.

\begin{figure}[ht]
\centering
\fbox{
\mbox{
\includegraphics[width=1.05\textwidth]{footballML.png}
}}
\smallskip
\caption{Some labeled examples, divided into training and test sets.}
\label{fig:labeledTrainTest}
\end{figure}

\index{training data}
\index{test data}

You'll also see in the figure that I've split the rows up into two groups. The
first group is called the \textbf{training data}, and the second, the
\textbf{test data}. (Normally we'll shuffle all the rows before assigning them,
so that we don't put all the top rows of the \texttt{DataFrame} in the training
set and all the bottom ones in the test set. But that's harder to show in a
picture.)

\section{Three kinds of examples}

Now here's the deal. There are \textit{three} kinds of example rows we're going
to deal with:

\begin{compactenum}
\item \textbf{training data} -- \textit{labeled} examples which we will show to
our classifier, and from which it will try to find useful patterns to make
future predictions.
\item \textbf{test data} -- \textit{labeled} examples which we will
\textit{not} show to
our classifier, but which we will use to measure how well it performs.
\item \textbf{new data} -- \textit{unlabeled} examples that we will get in the
future, after we've deployed our classifier in the field, and which we will
feed to our classifier to make predictions.
\end{compactenum}

The purpose of the first group is to give the classifier useful information so
it can intelligently classify.

The purpose of the second group is \textit{to assess how good the classifier's
predictions are.} Since the test set consists of labeled examples, we know the
``true answer'' for each one. So we can feed each of these test points to the
classifier, look at its prediction, compare it to the true answer, and judge
whether or not the classifier got it right. Assessing its accuracy is usually
just a matter of computing the percentage of how many test points it got right.

The third group exists because after we've built and evaluated our classifier,
we actually want to put it into action! These are new data points (new sporting
goods customers, say) for which we don't know the ``true answer'' but want to
predict it so we can send catalogs likely to be well-received.

\subsection{Thou shalt not reuse}

Now one common question -- which leads to a \textit{super} important point --
is this: why can't we use \textit{all} the labeled examples as training data?
After all, if we have 1000 labeled examples we've had to work hard (or pay
\$\$) to get, it seems silly to only use some of them to train our classifier.
Shouldn't we want to give it all the labeled data possible, so it can learn the
maximum amount before predicting?

The first reply is: ``but then we wouldn't have any test data, and so we
wouldn't know how good our classifier \textit{was} before putting it out in the
field.'' Clearly, before we base major business decisions on the results of our
automated predictor, we need to have some idea of how accurate its predictions
are.

It's then commonly countered: ``well, sure, but why not then re-use those data
points for testing? Instead of splitting the 1000 examples into training points
and test points, why not just use all 1000 for training, and then test the
classifier on all 1000 points? What's not to like?''

This is where the super important point comes in, and it's so important that
I'll put it all in boldface. It turns out that \textbf{you absolutely
\textit{cannot} test your classifier on data points that you gave it to train
on, because you will get an overly optimistic estimate of how good your
classifier actually is.}

Here's an analogy to make this point more clear. Suppose there's a final exam
coming up in your class, and your professor distributes a ``sample exam'' a
week before exam day for you to study from. This is a reasonable thing to do.
As long as the questions on the sample exam are of the same type and difficulty
as the ones that will appear on the actual final, you'll learn lots about what
the professor expects you to know from taking the sample exam. And you'll
probably increase your actual exam score, since this will help you master
exactly the right material.

But suppose the professor uses the \textit{exact same} exam for both the sample
exam and the actual final exam? Sure, the students would be ecstatic, but
that's not the point. The point is: \textit{in this case, students wouldn't
even have to learn the material.} They could simply memorize the answers! And
after they all get their A's back, they might be tempted to think they're
really great at chemistry...but they probably aren't. They're probably just
really great at memorizing and regurgitating.

Going from ``the kinds of questions you may be asked'' to ``\textit{exactly}
the questions you \textit{will} be asked'' makes all the difference. And if you
just studied the sample exam by memorization, and were then asked (surprise!)
to demonstrate your understanding of the material on a \textit{new} exam, you'd
probably suck it up.

\index{generalizing (to new data)}

And so, the absolute iron-clad rule is this: \textbf{any data that is given to
the classifier to learn from must \textit{not} be used to test it.} The test
data must be comprised of representative, but different, examples. It's the
only way to assess how well the classifier \textbf{generalizes} to new data
that it hasn't yet seen (which, of course, is the whole point).


\subsection{Splitting the difference}

Okay, so given that we have to split our precious labeled examples into two
sets, one for training and one for testing, how much do we devote to each? It
turns out that there are some sophisticated techniques (beyond the scope of
this book, but stay tuned for Volume II) in which we can cleverly re-use
portions of the data for different purposes, and effectively make use of nearly
all of it for training.

\index{rule of thumb (for training/test data)}
But for our introductory approach here, we'll just use a \textbf{rule of thumb:
70\% for training data, and the other 30\% for test data}.

\index{shuffle (\texttt{DataFrame} rows)}
\index{randomization!of training and test data}
As I mentioned earlier, we'll normally \textbf{shuffle} the rows randomly
before dividing them into these two groups, just in case there's any pattern to
the order in which they appear. For example, in our NFL fan data set, it might
turn out that the data came to us sequenced in a way such that people living on
the east coast were at the beginning of the \texttt{DataFrame} and those living
out west were at the end. Any arrangement like this would spell doom for our
classification endeavor. For one thing, we wouldn't be training on any west
coast people, and so our classifier would be oblivious to what those data
points looked like. For another thing, we'd \textit{only} be using west
coasters to \textit{test} our classifier, meaning that whatever accuracy
measure we computed is likely to be way off. \textbf{Randomizing} the data is
the sure way around this.

\index{sample@\texttt{.sample()} (Pandas)}

Here's some code to create training and test sets. The \texttt{.sample()}
method of a \texttt{DataFrame} lets you choose some percentage of its rows 
randomly. Its \texttt{frac} argument is a number between 0 and 1 and specifies
what fraction of the rows you want. Using the above rule of thumb, let's choose
70\% of them for our training data:

\begin{Verbatim}[fontsize=\small,samepage=true,frame=single,framesep=3mm]
training = fans.sample(frac=.7)
print(training)
\end{Verbatim}
\vspace{-.2in}

\begin{Verbatim}[fontsize=\small,samepage=true,frame=leftline,framesep=5mm,framerule=1mm]
    age        hometown current_residence  years_in_residence      team
8    52       Arlington    Fredericksburg                  17  Redskins
15   63        New York           Orlando                  17    Giants
11   29  Fredericksburg           Seattle                  21  Seahawks
19   32       Warrenton           Orlando                   1  Redskins
2    47       Allegheny       Chevy Chase                  13  Redskins
7    31       Warrenton   Charlottesville                   6      Jets
0    29       Warrenton         Warrenton                  29  Redskins
5    32       Warrenton        Winchester                  11  Redskins
4    51   Orange County          Dumfries                  23  Redskins
9    60  Tyson's Corner      Falls Church                   4   Cowboys
10   17  Fredericksburg    Fredericksburg                  17  Redskins
1    39         Lubbock         Arlington                   2   Cowboys
13   35  Mechanicsville           Orlando                   8  Redskins
6    39        Dumfries             Miami                   5  Redskins
\end{Verbatim}

Notice that the numeric index values (far left) are in no particular order,
since that's the point of taking a random sample. Also notice that there are
only 14 rows in this \texttt{DataFrame} instead of the full 20 that were in
\texttt{fans}.

\index{not (query condition)}
\index{squiggle (tilde, or ``\texttt{\textasciitilde}'')}
\index{~@\texttt{\textasciitilde} (squiggle)}
\index{query}
\index{index@\texttt{.index} (little \texttt{i}) syntax}

Now, we want our test set. The trick here is to say: ``give me all the rows of
\texttt{fans} that were \textit{not} selected for the \texttt{training} set.''
By building a query with the squiggle operator (``\texttt{\textasciitilde}'',
meaning ``not'') in conjunction with the ``\texttt{.isin()}'' method, we can
create a new \texttt{DataFrame} called ``\texttt{test}'' that has exactly these
rows:

\begin{Verbatim}[fontsize=\small,samepage=true,frame=single,framesep=3mm]
test = fans[~fans.index.isin(training.index)]
print(test)
\end{Verbatim}
\vspace{-.2in}

\begin{Verbatim}[fontsize=\small,samepage=true,frame=leftline,framesep=5mm,framerule=1mm]
    age        hometown current_residence  years_in_residence      team
3    31       Littleton         Littleton                   3   Broncos
12   37        Richmond          Richmond                  37  Redskins
14   19        New York          New York                  19    Giants
16   42      Alexandria          Manassas                  10  Redskins
17   22       Allegheny        Harrisburg                   1  Steelers
18   38  Mechanicsville    Fredericksburg                   6  Redskins
\end{Verbatim}

That code says, in English: ``create a new variable \texttt{test} that contains
only those rows of \texttt{fans} whose index is \textit{not} present in any of
the \texttt{training} \texttt{DataFrame}'s indices.'' As you can verify through
visual inspection, the result does have exactly the 6 rows that were missing
from \texttt{training}.

\section{``The prior''}

\label{prior}
\index{prior@``the prior''}

One more piece of lingo before we dive into a particular classification
technique next chapter. And that's known as ``the \textbf{prior}'' of a data
set.

\index{Bayesian reasoning}
\index{posterior@posterior}

The term comes from something called \textbf{Bayesian reasoning}, which is a
whole subject (and a super cool one!) in its own right. All you need to know
here is the concept of two different quantities: the \textbf{prior}, and the
\textbf{posterior}.

In common usage, the word ``prior'' means ``beforehand,'' and so it does here:
the prior is your best judgment about what the target value of a new example
might be \textit{before you actually look at the feature values in that
example.} ``Posterior,'' on the other hand, means ``afterwards,'' and means
your best judgment about the target value after duly taking into consideration
all the feature values.

For example, you may have noticed that in my made-up data set, above, I had a
lot of Redskins fans. This is because I live in the D.C.~area, and happen to
know a lot of Redskins fans. Out of my 20 labeled examples, a whopping twelve
of them, in fact, had \texttt{Redskins} as their value in the \texttt{team}
column.

Thus, consider the following question. Suppose you knew nothing about a person
except that they were one of Stephen's friends. Which NFL team do you think
they'd support? Assuming this data set is representative of Stephen's friends,
you'd say: ``I'd predict they'd be a Redskins fan, and I'd estimate that I'd
have about a 60\% chance of being right ($\frac{12}{20}$).'' This is the
\textit{prior}. You're not taking into account anything about their age, where
they were born, \textit{etc.}; in fact, you weren't even told those things.
Instead, you're just ``using the prior'' and treating everyone the same.

It would be a different story if I told you that this person was born in New
York City. Then you might squint your eyes at my data set and realize that
there are only two New Yorkers in it, and neither one is a Redskins fan:
they're both Giants fans! Now you might very well move away from your prior
assumption. ``Sure, most of Stephen's friends are Redskins fans, so `Redskins'
is a reasonable guess, but now that you've told me they're from NY, that very
well might change my mind. Now, my guess is `Giants'.''

I keep saying ``might'' and ``may'' because different kinds of classifiers work
in different ways. Some of them may choose to take advantage of some features
but not others; some may just stick with the prior in certain situations. The
notion of ``the prior'' is mainly useful as a baseline for comparison: it's the
best you can do given no other possibly correlating information. The name of
the game in classification, of course, is to intelligently \textit{use} that
other information to make more informed guesses, and to beat the prior. One of
many ways to approach this is the decision tree classification algorithm, which
we'll look at in detail next.

