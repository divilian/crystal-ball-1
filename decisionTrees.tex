
\chapter{Decision trees for classification}

So far our classification picture has been very general. We haven't said
anything about how our classifier might actually \textit{work}; we've just said
that given values for each of the features, it will render a prediction about
what the label will be.

\index{decision tree}

In this chapter, we'll study one particular algorithm for classification in
machine learning: the \textbf{decision tree} algorithm.

\section{A working example}

\index{catalog} \index{videogames}
Here's a (fictitious) domain problem that we'll use to demonstrate the
principles in this chapter. Say we own a videogame business, and we want to
send full-color product catalogs to unsuspecting college students, so that they
will buy our games and keep us in business (while meanwhile failing out of
school due to playing all the time).

Now full-color catalogs are expensive to print and ship, so we want to be smart
about this. We definitely don't want to send a bunch of catalogs to students
who aren't likely buyers; that would run our business into the ground. Instead,
we'd like to identify the subset of students who probably gamers, and send
catalogs to only \textit{those} students.

Suppose that through nefarious means, we have acquired the following data set:

\begin{Verbatim}[fontsize=\small,samepage=true,frame=single,framesep=3mm,xleftmargin=4.3cm,xrightmargin=4.2cm]
   major  age gender   VG
0   PSYC   22      F   No
1   MATH   20      F   No
2   PSYC   19      F   No
3   CPSC   20      M  Yes
4   MATH   18      M  Yes
5   CPSC   20      F   No
6   CPSC   19      O   No
7   CPSC   17      M  Yes
8   PSYC   18      F   No
9   CPSC   20      F   No
10  MATH   18      F   No
11  CPSC   22      F  Yes
12  MATH   21      M   No
13  CPSC   23      M  Yes
14  PSYC   17      M  Yes
15  CPSC   18      F   No
16  PSYC   19      F  Yes
\end{Verbatim}

\index{Psychology}
\index{Mathematics}
\index{Computer Science}

Each row represents one college student, with three features. The first is
their major -- \texttt{PSYC} (Psychology), \texttt{MATH} (Mathematics), or
\texttt{CPSC} (Computer Science). (For simplicity, we'll say these are the only
three possibilities, since your author happens to like them the best.) The
second is their age (numeric), and the third is their gender: male, female, or
other. The last column is our target: \textit{whether or not this student is a
videogamer.} Glance over this \texttt{DataFrame} for a moment.

\subsection{Eyeing the prior}

\index{value\_counts@\texttt{.value\_counts()} method (Pandas)}

As you remember from section~\ref{prior}, before we even think about features,
we might as well the target variable itself, and ask ourselves ``given no other
information about a student, what would be our gut feel about their videogame
status?'' Our pal the \texttt{.value\_counts()} method is perfect to compute
this:

\begin{Verbatim}[fontsize=\small,samepage=true,frame=single,framesep=3mm]
print(students.VG.value_counts())
\end{Verbatim}
\vspace{-.2in}

\begin{Verbatim}[fontsize=\small,samepage=true,frame=leftline,framesep=5mm,framerule=1mm]
N    10
Y     7
Name: VG, dtype: int64
\end{Verbatim}

So if we're smart, we'd guess ``no'' for such mysterious persons, but we could
only expect to be right about $\frac{10}{17}^\textrm{ths}$, or 59\%, of the
time.

\subsection{Sticking with categorical features}

\index{categorical variable}

Now it turns out that decision trees work best with all categorical features,
not a mix of categorical and numeric. So for now, we're going to simply
classify each of our students into three buckets: ``\texttt{young}'' (18 or
younger), ``\texttt{middle}'' (19-21), and ``\texttt{old}''
(22+).\footnote{\index{Swift, Taylor} Believe it or not, a time will come in
your life when 22 years of age does not remotely seem ``old.'' For undergrads,
though, I can see why 22 would seem on the grey side, Taylor Swift's song
notwithstanding.} For the moment, don't ask why we chose three age categories
instead of two or four, and don't ask why we chose those particular split
points. We just did. More on that later.

Our training data now looks like this:

\begin{Verbatim}[fontsize=\small,samepage=true,frame=single,framesep=3mm,xleftmargin=4cm,xrightmargin=4cm]
   major     age gender   VG
0   PSYC     old      F   No
1   MATH  middle      F   No
2   PSYC  middle      F   No
3   CPSC  middle      M  Yes
4   MATH   young      M  Yes
5   CPSC  middle      F   No
6   CPSC  middle      O   No
7   CPSC   young      M  Yes
8   PSYC   young      F   No
9   CPSC  middle      F   No
10  MATH   young      F   No
11  CPSC     old      F  Yes
12  MATH  middle      M   No
13  CPSC     old      M  Yes
14  PSYC   young      M  Yes
15  CPSC   young      F   No
16  PSYC  middle      F  Yes
\end{Verbatim}

and we're now officially ready to consider decision trees.


%
%
%I love the word ``training'' for this, by the way.
